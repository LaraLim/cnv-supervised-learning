{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lc_t1O7Ipm25"
      },
      "source": [
        "## Mapping CNV Segments to Genes\n",
        "\n",
        "### Overview\n",
        "This notebook uses PyEnsembl's genome metadata to look up genes based on the retrieved CNV data. Chromosome, Start, and End (genomic positions of CNV segment) are used to specify the locus. GRCh38, the most recent and comphrehensive human reference genome assembly, is used via PyEnsembl as well.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "93D7TtWdjSri"
      },
      "source": [
        "PyEnsembl installation and reference genome data (GRCh38) download"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "CN2jBUrEP8Zb",
        "outputId": "3302c38a-6bd2-4f3b-9b1b-fe75feb9ca80"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyensembl\n",
            "  Downloading pyensembl-2.3.13-py3-none-any.whl.metadata (9.4 kB)\n",
            "Collecting typechecks<1.0.0,>=0.0.2 (from pyensembl)\n",
            "  Downloading typechecks-0.1.0.tar.gz (3.4 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting datacache<2.0.0,>=1.4.0 (from pyensembl)\n",
            "  Downloading datacache-1.4.1-py3-none-any.whl.metadata (1.9 kB)\n",
            "Collecting memoized-property>=1.0.2 (from pyensembl)\n",
            "  Downloading memoized-property-1.0.3.tar.gz (5.0 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting tinytimer<1.0.0,>=0.0.0 (from pyensembl)\n",
            "  Downloading tinytimer-0.0.0.tar.gz (2.1 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting gtfparse<3.0.0,>=2.5.0 (from pyensembl)\n",
            "  Downloading gtfparse-2.5.0-py3-none-any.whl.metadata (2.1 kB)\n",
            "Collecting serializable<1.0.0,>=0.2.1 (from pyensembl)\n",
            "  Downloading serializable-0.4.1-py3-none-any.whl.metadata (2.2 kB)\n",
            "Collecting pylint<3.0.0,>=2.17.2 (from pyensembl)\n",
            "  Downloading pylint-2.17.7-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: pandas>=0.15.2 in /usr/local/lib/python3.10/dist-packages (from datacache<2.0.0,>=1.4.0->pyensembl) (2.2.2)\n",
            "Collecting appdirs>=1.4.0 (from datacache<2.0.0,>=1.4.0->pyensembl)\n",
            "  Downloading appdirs-1.4.4-py2.py3-none-any.whl.metadata (9.0 kB)\n",
            "Collecting progressbar33>=2.4 (from datacache<2.0.0,>=1.4.0->pyensembl)\n",
            "  Downloading progressbar33-2.4.tar.gz (10 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: requests>=2.5.1 in /usr/local/lib/python3.10/dist-packages (from datacache<2.0.0,>=1.4.0->pyensembl) (2.32.3)\n",
            "Collecting mock (from datacache<2.0.0,>=1.4.0->pyensembl)\n",
            "  Downloading mock-5.1.0-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting polars<0.21.0,>=0.20.2 (from gtfparse<3.0.0,>=2.5.0->pyensembl)\n",
            "  Downloading polars-0.20.31-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (14 kB)\n",
            "Collecting pyarrow<14.1.0,>=14.0.2 (from gtfparse<3.0.0,>=2.5.0->pyensembl)\n",
            "  Downloading pyarrow-14.0.2-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: platformdirs>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from pylint<3.0.0,>=2.17.2->pyensembl) (4.3.6)\n",
            "Collecting astroid<=2.17.0-dev0,>=2.15.8 (from pylint<3.0.0,>=2.17.2->pyensembl)\n",
            "  Downloading astroid-2.15.8-py3-none-any.whl.metadata (4.7 kB)\n",
            "Collecting isort<6,>=4.2.5 (from pylint<3.0.0,>=2.17.2->pyensembl)\n",
            "  Downloading isort-5.13.2-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting mccabe<0.8,>=0.6 (from pylint<3.0.0,>=2.17.2->pyensembl)\n",
            "  Downloading mccabe-0.7.0-py2.py3-none-any.whl.metadata (5.0 kB)\n",
            "Collecting tomlkit>=0.10.1 (from pylint<3.0.0,>=2.17.2->pyensembl)\n",
            "  Downloading tomlkit-0.13.2-py3-none-any.whl.metadata (2.7 kB)\n",
            "Collecting dill>=0.2 (from pylint<3.0.0,>=2.17.2->pyensembl)\n",
            "  Downloading dill-0.3.9-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: tomli>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from pylint<3.0.0,>=2.17.2->pyensembl) (2.0.2)\n",
            "Collecting simplejson (from serializable<1.0.0,>=0.2.1->pyensembl)\n",
            "  Downloading simplejson-3.19.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.2 kB)\n",
            "Collecting lazy-object-proxy>=1.4.0 (from astroid<=2.17.0-dev0,>=2.15.8->pylint<3.0.0,>=2.17.2->pyensembl)\n",
            "  Downloading lazy_object_proxy-1.10.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.8 kB)\n",
            "Requirement already satisfied: wrapt<2,>=1.11 in /usr/local/lib/python3.10/dist-packages (from astroid<=2.17.0-dev0,>=2.15.8->pylint<3.0.0,>=2.17.2->pyensembl) (1.16.0)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from astroid<=2.17.0-dev0,>=2.15.8->pylint<3.0.0,>=2.17.2->pyensembl) (4.12.2)\n",
            "Requirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.15.2->datacache<2.0.0,>=1.4.0->pyensembl) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.15.2->datacache<2.0.0,>=1.4.0->pyensembl) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.15.2->datacache<2.0.0,>=1.4.0->pyensembl) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.15.2->datacache<2.0.0,>=1.4.0->pyensembl) (2024.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.5.1->datacache<2.0.0,>=1.4.0->pyensembl) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.5.1->datacache<2.0.0,>=1.4.0->pyensembl) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.5.1->datacache<2.0.0,>=1.4.0->pyensembl) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.5.1->datacache<2.0.0,>=1.4.0->pyensembl) (2024.8.30)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas>=0.15.2->datacache<2.0.0,>=1.4.0->pyensembl) (1.16.0)\n",
            "Downloading pyensembl-2.3.13-py3-none-any.whl (55 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.0/56.0 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading datacache-1.4.1-py3-none-any.whl (20 kB)\n",
            "Downloading gtfparse-2.5.0-py3-none-any.whl (15 kB)\n",
            "Downloading pylint-2.17.7-py3-none-any.whl (537 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m537.2/537.2 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading serializable-0.4.1-py3-none-any.whl (12 kB)\n",
            "Downloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
            "Downloading astroid-2.15.8-py3-none-any.whl (278 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m278.3/278.3 kB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.9-py3-none-any.whl (119 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.4/119.4 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading isort-5.13.2-py3-none-any.whl (92 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.3/92.3 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mccabe-0.7.0-py2.py3-none-any.whl (7.3 kB)\n",
            "Downloading polars-0.20.31-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (28.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m28.8/28.8 MB\u001b[0m \u001b[31m46.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyarrow-14.0.2-cp310-cp310-manylinux_2_28_x86_64.whl (38.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.0/38.0 MB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tomlkit-0.13.2-py3-none-any.whl (37 kB)\n",
            "Downloading mock-5.1.0-py3-none-any.whl (30 kB)\n",
            "Downloading simplejson-3.19.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (137 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.9/137.9 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lazy_object_proxy-1.10.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (68 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.3/68.3 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: memoized-property, tinytimer, typechecks, progressbar33\n",
            "  Building wheel for memoized-property (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for memoized-property: filename=memoized_property-1.0.3-py2.py3-none-any.whl size=4185 sha256=9e2ed916fa853692d1e67670db4bd5d4122d5ca23e06b55b5d64bc58ba0bb992\n",
            "  Stored in directory: /root/.cache/pip/wheels/f7/0f/93/d0497c1248dcc9113cfa09e6ddf52f5636b68bcd6751bad581\n",
            "  Building wheel for tinytimer (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tinytimer: filename=tinytimer-0.0.0-py3-none-any.whl size=1874 sha256=386739672f3f7452de0ffa0c759afa325824a2411ed9a9977908d43594d850e8\n",
            "  Stored in directory: /root/.cache/pip/wheels/35/7f/a3/036dd96fcd3da5d509047bcc764790cea8f6c73edadb1cbedc\n",
            "  Building wheel for typechecks (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for typechecks: filename=typechecks-0.1.0-py3-none-any.whl size=2752 sha256=ab78be47240e6a47dcec9b7481bb1a83467cdb7752f0ca366dc93b019b93d114\n",
            "  Stored in directory: /root/.cache/pip/wheels/da/a7/4d/6fb19c879271cf359a24d5bc4d074d53380cc2473f70068c4e\n",
            "  Building wheel for progressbar33 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for progressbar33: filename=progressbar33-2.4-py3-none-any.whl size=12142 sha256=925f16501a77891bac8ff6aaac530549daaa0434082d5e189fc907d218e75228\n",
            "  Stored in directory: /root/.cache/pip/wheels/28/6d/b6/fd4b9b1489353528a2eea1200c98532fd60d535e22498d9e28\n",
            "Successfully built memoized-property tinytimer typechecks progressbar33\n",
            "Installing collected packages: typechecks, tinytimer, progressbar33, memoized-property, appdirs, tomlkit, simplejson, pyarrow, polars, mock, mccabe, lazy-object-proxy, isort, dill, serializable, gtfparse, astroid, pylint, datacache, pyensembl\n",
            "  Attempting uninstall: pyarrow\n",
            "    Found existing installation: pyarrow 16.1.0\n",
            "    Uninstalling pyarrow-16.1.0:\n",
            "      Successfully uninstalled pyarrow-16.1.0\n",
            "  Attempting uninstall: polars\n",
            "    Found existing installation: polars 1.7.1\n",
            "    Uninstalling polars-1.7.1:\n",
            "      Successfully uninstalled polars-1.7.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "cudf-cu12 24.6.1 requires pyarrow<16.2.0a0,>=16.1.0, but you have pyarrow 14.0.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed appdirs-1.4.4 astroid-2.15.8 datacache-1.4.1 dill-0.3.9 gtfparse-2.5.0 isort-5.13.2 lazy-object-proxy-1.10.0 mccabe-0.7.0 memoized-property-1.0.3 mock-5.1.0 polars-0.20.31 progressbar33-2.4 pyarrow-14.0.2 pyensembl-2.3.13 pylint-2.17.7 serializable-0.4.1 simplejson-3.19.3 tinytimer-0.0.0 tomlkit-0.13.2 typechecks-0.1.0\n",
            "2024-10-17 06:53:36,624 - pyensembl.shell - INFO - Running 'install' for EnsemblRelease(release=84, species='homo_sapiens')\n",
            "2024-10-17 06:53:36,624 - pyensembl.download_cache - INFO - Fetching /root/.cache/pyensembl/GRCh38/ensembl84/Homo_sapiens.GRCh38.84.gtf.gz from URL https://ftp.ensembl.org/pub/release-84/gtf/homo_sapiens/Homo_sapiens.GRCh38.84.gtf.gz\n",
            "2024-10-17 06:53:36,625 - datacache.download - INFO - Downloading https://ftp.ensembl.org/pub/release-84/gtf/homo_sapiens/Homo_sapiens.GRCh38.84.gtf.gz to /root/.cache/pyensembl/GRCh38/ensembl84/Homo_sapiens.GRCh38.84.gtf.gz\n",
            "2024-10-17 06:54:39,553 - pyensembl.download_cache - INFO - Fetching /root/.cache/pyensembl/GRCh38/ensembl84/Homo_sapiens.GRCh38.cdna.all.fa.gz from URL https://ftp.ensembl.org/pub/release-84/fasta/homo_sapiens/cdna/Homo_sapiens.GRCh38.cdna.all.fa.gz\n",
            "2024-10-17 06:54:39,554 - datacache.download - INFO - Downloading https://ftp.ensembl.org/pub/release-84/fasta/homo_sapiens/cdna/Homo_sapiens.GRCh38.cdna.all.fa.gz to /root/.cache/pyensembl/GRCh38/ensembl84/Homo_sapiens.GRCh38.cdna.all.fa.gz\n",
            "2024-10-17 06:56:07,120 - pyensembl.download_cache - INFO - Fetching /root/.cache/pyensembl/GRCh38/ensembl84/Homo_sapiens.GRCh38.ncrna.fa.gz from URL https://ftp.ensembl.org/pub/release-84/fasta/homo_sapiens/ncrna/Homo_sapiens.GRCh38.ncrna.fa.gz\n",
            "2024-10-17 06:56:07,121 - datacache.download - INFO - Downloading https://ftp.ensembl.org/pub/release-84/fasta/homo_sapiens/ncrna/Homo_sapiens.GRCh38.ncrna.fa.gz to /root/.cache/pyensembl/GRCh38/ensembl84/Homo_sapiens.GRCh38.ncrna.fa.gz\n",
            "2024-10-17 06:56:20,408 - pyensembl.download_cache - INFO - Fetching /root/.cache/pyensembl/GRCh38/ensembl84/Homo_sapiens.GRCh38.pep.all.fa.gz from URL https://ftp.ensembl.org/pub/release-84/fasta/homo_sapiens/pep/Homo_sapiens.GRCh38.pep.all.fa.gz\n",
            "2024-10-17 06:56:20,409 - datacache.download - INFO - Downloading https://ftp.ensembl.org/pub/release-84/fasta/homo_sapiens/pep/Homo_sapiens.GRCh38.pep.all.fa.gz to /root/.cache/pyensembl/GRCh38/ensembl84/Homo_sapiens.GRCh38.pep.all.fa.gz\n",
            "2024-10-17 06:56:39,790 - pyensembl.database - INFO - Creating database: /root/.cache/pyensembl/GRCh38/ensembl84/Homo_sapiens.GRCh38.84.gtf.db\n",
            "2024-10-17 06:56:39,791 - pyensembl.database - INFO - Reading GTF from /root/.cache/pyensembl/GRCh38/ensembl84/Homo_sapiens.GRCh38.84.gtf.gz\n",
            "2024-10-17 06:58:44,801 - datacache.database_helpers - INFO - Creating database /root/.cache/pyensembl/GRCh38/ensembl84/Homo_sapiens.GRCh38.84.gtf.db containing: exon, CDS, stop_codon, gene, start_codon, transcript\n",
            "2024-10-17 06:58:44,802 - datacache.database - INFO - Running sqlite query: \"CREATE TABLE exon (ccds_id TEXT NOT NULL, start INT NOT NULL, protein_version TEXT NOT NULL, transcript_biotype TEXT NOT NULL, transcript_version TEXT NOT NULL, transcript_id TEXT NOT NULL, gene_name TEXT NOT NULL, gene_version TEXT NOT NULL, source TEXT NOT NULL, transcript_support_level TEXT NOT NULL, protein_id TEXT NOT NULL, seqname TEXT NOT NULL, feature TEXT NOT NULL, exon_id TEXT NOT NULL, strand TEXT NOT NULL, end INT NOT NULL, gene_biotype TEXT NOT NULL, exon_version TEXT NOT NULL, transcript_name TEXT NOT NULL, exon_number TEXT NOT NULL, gene_id TEXT NOT NULL)\"\n",
            "2024-10-17 06:58:48,367 - datacache.database - INFO - Inserting 1176808 rows into table exon\n",
            "2024-10-17 06:58:56,176 - datacache.database - INFO - Creating index on exon (seqname, start, end)\n",
            "2024-10-17 06:58:57,408 - datacache.database - INFO - Creating index on exon (gene_name)\n",
            "2024-10-17 06:58:58,352 - datacache.database - INFO - Creating index on exon (gene_id)\n",
            "2024-10-17 06:58:59,356 - datacache.database - INFO - Creating index on exon (transcript_id)\n",
            "2024-10-17 06:59:00,441 - datacache.database - INFO - Creating index on exon (transcript_name)\n",
            "2024-10-17 06:59:02,158 - datacache.database - INFO - Creating index on exon (exon_id)\n",
            "2024-10-17 06:59:04,242 - datacache.database - INFO - Creating index on exon (protein_id)\n",
            "2024-10-17 06:59:04,827 - datacache.database - INFO - Creating index on exon (ccds_id)\n",
            "2024-10-17 06:59:05,572 - datacache.database - INFO - Running sqlite query: \"CREATE TABLE CDS (ccds_id TEXT NOT NULL, start INT NOT NULL, protein_version TEXT NOT NULL, transcript_biotype TEXT NOT NULL, transcript_version TEXT NOT NULL, transcript_id TEXT NOT NULL, gene_name TEXT NOT NULL, gene_version TEXT NOT NULL, source TEXT NOT NULL, transcript_support_level TEXT NOT NULL, protein_id TEXT NOT NULL, seqname TEXT NOT NULL, feature TEXT NOT NULL, exon_id TEXT NOT NULL, strand TEXT NOT NULL, end INT NOT NULL, gene_biotype TEXT NOT NULL, exon_version TEXT NOT NULL, transcript_name TEXT NOT NULL, exon_number TEXT NOT NULL, gene_id TEXT NOT NULL)\"\n",
            "2024-10-17 06:59:07,643 - datacache.database - INFO - Inserting 702410 rows into table CDS\n",
            "2024-10-17 06:59:11,786 - datacache.database - INFO - Creating index on CDS (seqname, start, end)\n",
            "2024-10-17 06:59:12,600 - datacache.database - INFO - Creating index on CDS (gene_name)\n",
            "2024-10-17 06:59:13,092 - datacache.database - INFO - Creating index on CDS (gene_id)\n",
            "2024-10-17 06:59:13,668 - datacache.database - INFO - Creating index on CDS (transcript_id)\n",
            "2024-10-17 06:59:14,429 - datacache.database - INFO - Creating index on CDS (transcript_name)\n",
            "2024-10-17 06:59:15,320 - datacache.database - INFO - Creating index on CDS (exon_id)\n",
            "2024-10-17 06:59:15,969 - datacache.database - INFO - Creating index on CDS (protein_id)\n",
            "2024-10-17 06:59:16,855 - datacache.database - INFO - Creating index on CDS (ccds_id)\n",
            "2024-10-17 06:59:17,275 - datacache.database - INFO - Running sqlite query: \"CREATE TABLE stop_codon (ccds_id TEXT NOT NULL, start INT NOT NULL, protein_version TEXT NOT NULL, transcript_biotype TEXT NOT NULL, transcript_version TEXT NOT NULL, transcript_id TEXT NOT NULL, gene_name TEXT NOT NULL, gene_version TEXT NOT NULL, source TEXT NOT NULL, transcript_support_level TEXT NOT NULL, protein_id TEXT NOT NULL, seqname TEXT NOT NULL, feature TEXT NOT NULL, exon_id TEXT NOT NULL, strand TEXT NOT NULL, end INT NOT NULL, gene_biotype TEXT NOT NULL, exon_version TEXT NOT NULL, transcript_name TEXT NOT NULL, exon_number TEXT NOT NULL, gene_id TEXT NOT NULL)\"\n",
            "2024-10-17 06:59:17,477 - datacache.database - INFO - Inserting 73802 rows into table stop_codon\n",
            "2024-10-17 06:59:17,897 - datacache.database - INFO - Creating index on stop_codon (seqname, start, end)\n",
            "2024-10-17 06:59:17,950 - datacache.database - INFO - Creating index on stop_codon (gene_name)\n",
            "2024-10-17 06:59:18,001 - datacache.database - INFO - Creating index on stop_codon (gene_id)\n",
            "2024-10-17 06:59:18,078 - datacache.database - INFO - Creating index on stop_codon (transcript_id)\n",
            "2024-10-17 06:59:18,150 - datacache.database - INFO - Creating index on stop_codon (transcript_name)\n",
            "2024-10-17 06:59:18,236 - datacache.database - INFO - Creating index on stop_codon (exon_id)\n",
            "2024-10-17 06:59:18,266 - datacache.database - INFO - Creating index on stop_codon (protein_id)\n",
            "2024-10-17 06:59:18,296 - datacache.database - INFO - Creating index on stop_codon (ccds_id)\n",
            "2024-10-17 06:59:18,341 - datacache.database - INFO - Running sqlite query: \"CREATE TABLE gene (ccds_id TEXT NOT NULL, start INT NOT NULL, protein_version TEXT NOT NULL, transcript_biotype TEXT NOT NULL, transcript_version TEXT NOT NULL, transcript_id TEXT NOT NULL, gene_name TEXT NOT NULL, gene_version TEXT NOT NULL, source TEXT NOT NULL, transcript_support_level TEXT NOT NULL, protein_id TEXT NOT NULL, seqname TEXT NOT NULL, feature TEXT NOT NULL, exon_id TEXT NOT NULL, strand TEXT NOT NULL, end INT NOT NULL, gene_biotype TEXT NOT NULL, exon_version TEXT NOT NULL, transcript_name TEXT NOT NULL, exon_number TEXT NOT NULL, gene_id TEXT UNIQUE PRIMARY KEY NOT NULL)\"\n",
            "2024-10-17 06:59:18,491 - datacache.database - INFO - Inserting 60675 rows into table gene\n",
            "2024-10-17 06:59:18,955 - datacache.database - INFO - Creating index on gene (seqname, start, end)\n",
            "2024-10-17 06:59:19,000 - datacache.database - INFO - Creating index on gene (gene_name)\n",
            "2024-10-17 06:59:19,052 - datacache.database - INFO - Creating index on gene (transcript_id)\n",
            "2024-10-17 06:59:19,074 - datacache.database - INFO - Creating index on gene (transcript_name)\n",
            "2024-10-17 06:59:19,100 - datacache.database - INFO - Creating index on gene (exon_id)\n",
            "2024-10-17 06:59:19,125 - datacache.database - INFO - Creating index on gene (protein_id)\n",
            "2024-10-17 06:59:19,149 - datacache.database - INFO - Creating index on gene (ccds_id)\n",
            "2024-10-17 06:59:19,170 - datacache.database - INFO - Running sqlite query: \"CREATE TABLE start_codon (ccds_id TEXT NOT NULL, start INT NOT NULL, protein_version TEXT NOT NULL, transcript_biotype TEXT NOT NULL, transcript_version TEXT NOT NULL, transcript_id TEXT NOT NULL, gene_name TEXT NOT NULL, gene_version TEXT NOT NULL, source TEXT NOT NULL, transcript_support_level TEXT NOT NULL, protein_id TEXT NOT NULL, seqname TEXT NOT NULL, feature TEXT NOT NULL, exon_id TEXT NOT NULL, strand TEXT NOT NULL, end INT NOT NULL, gene_biotype TEXT NOT NULL, exon_version TEXT NOT NULL, transcript_name TEXT NOT NULL, exon_number TEXT NOT NULL, gene_id TEXT NOT NULL)\"\n",
            "2024-10-17 06:59:19,458 - datacache.database - INFO - Inserting 82403 rows into table start_codon\n",
            "2024-10-17 06:59:19,949 - datacache.database - INFO - Creating index on start_codon (seqname, start, end)\n",
            "2024-10-17 06:59:20,021 - datacache.database - INFO - Creating index on start_codon (gene_name)\n",
            "2024-10-17 06:59:20,081 - datacache.database - INFO - Creating index on start_codon (gene_id)\n",
            "2024-10-17 06:59:20,149 - datacache.database - INFO - Creating index on start_codon (transcript_id)\n",
            "2024-10-17 06:59:20,226 - datacache.database - INFO - Creating index on start_codon (transcript_name)\n",
            "2024-10-17 06:59:20,290 - datacache.database - INFO - Creating index on start_codon (exon_id)\n",
            "2024-10-17 06:59:20,324 - datacache.database - INFO - Creating index on start_codon (protein_id)\n",
            "2024-10-17 06:59:20,357 - datacache.database - INFO - Creating index on start_codon (ccds_id)\n",
            "2024-10-17 06:59:20,415 - datacache.database - INFO - Running sqlite query: \"CREATE TABLE transcript (ccds_id TEXT NOT NULL, start INT NOT NULL, protein_version TEXT NOT NULL, transcript_biotype TEXT NOT NULL, transcript_version TEXT NOT NULL, transcript_id TEXT UNIQUE PRIMARY KEY NOT NULL, gene_name TEXT NOT NULL, gene_version TEXT NOT NULL, source TEXT NOT NULL, transcript_support_level TEXT NOT NULL, protein_id TEXT NOT NULL, seqname TEXT NOT NULL, feature TEXT NOT NULL, exon_id TEXT NOT NULL, strand TEXT NOT NULL, end INT NOT NULL, gene_biotype TEXT NOT NULL, exon_version TEXT NOT NULL, transcript_name TEXT NOT NULL, exon_number TEXT NOT NULL, gene_id TEXT NOT NULL)\"\n",
            "2024-10-17 06:59:21,054 - datacache.database - INFO - Inserting 199184 rows into table transcript\n",
            "2024-10-17 06:59:22,649 - datacache.database - INFO - Creating index on transcript (seqname, start, end)\n",
            "2024-10-17 06:59:22,815 - datacache.database - INFO - Creating index on transcript (gene_name)\n",
            "2024-10-17 06:59:22,967 - datacache.database - INFO - Creating index on transcript (gene_id)\n",
            "2024-10-17 06:59:23,139 - datacache.database - INFO - Creating index on transcript (transcript_name)\n",
            "2024-10-17 06:59:23,309 - datacache.database - INFO - Creating index on transcript (exon_id)\n",
            "2024-10-17 06:59:23,419 - datacache.database - INFO - Creating index on transcript (protein_id)\n",
            "2024-10-17 06:59:23,519 - datacache.database - INFO - Creating index on transcript (ccds_id)\n",
            "2024-10-17 06:59:23,642 - datacache.database - INFO - Running sqlite query: \"CREATE TABLE _datacache_metadata (version INT)\"\n",
            "2024-10-17 06:59:23,643 - datacache.database - INFO - Running sqlite query: \"INSERT INTO _datacache_metadata VALUES (3)\"\n",
            "2024-10-17 06:59:24,283 - pyensembl.sequence_data - INFO - Parsing sequences from FASTA file at /root/.cache/pyensembl/GRCh38/ensembl84/Homo_sapiens.GRCh38.cdna.all.fa.gz\n",
            "2024-10-17 06:59:32,850 - pyensembl.sequence_data - INFO - Saving sequence dictionary to /root/.cache/pyensembl/GRCh38/ensembl84/Homo_sapiens.GRCh38.cdna.all.fa.gz.pickle\n",
            "2024-10-17 06:59:33,804 - pyensembl.sequence_data - INFO - Parsing sequences from FASTA file at /root/.cache/pyensembl/GRCh38/ensembl84/Homo_sapiens.GRCh38.ncrna.fa.gz\n",
            "2024-10-17 06:59:34,753 - pyensembl.sequence_data - INFO - Saving sequence dictionary to /root/.cache/pyensembl/GRCh38/ensembl84/Homo_sapiens.GRCh38.ncrna.fa.gz.pickle\n",
            "2024-10-17 06:59:34,856 - pyensembl.sequence_data - INFO - Parsing sequences from FASTA file at /root/.cache/pyensembl/GRCh38/ensembl84/Homo_sapiens.GRCh38.pep.all.fa.gz\n",
            "2024-10-17 06:59:36,913 - pyensembl.sequence_data - INFO - Saving sequence dictionary to /root/.cache/pyensembl/GRCh38/ensembl84/Homo_sapiens.GRCh38.pep.all.fa.gz.pickle\n"
          ]
        }
      ],
      "source": [
        "!pip install pyensembl\n",
        "!pyensembl install --release 84 --species homo_sapiens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ugjYd3gSpVea"
      },
      "source": [
        "Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "X43CETyaPaJm"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from pyensembl import EnsemblRelease\n",
        "from google.colab import files"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get CNV data from uploaded files on Google Colab"
      ],
      "metadata": {
        "id": "5tHscl5LQedG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "uploaded = files.upload()\n",
        "df = pd.read_csv(\"ground_truth_combined.csv\", delimiter=',')\n",
        "print(df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 307
        },
        "id": "XjYkxCiIQe5L",
        "outputId": "97a74756-66f6-4c54-925f-461f578b1b98"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-90ab11b9-618e-471c-aa7f-9ee1c9a7a5c7\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-90ab11b9-618e-471c-aa7f-9ee1c9a7a5c7\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving ground_truth_combined.csv to ground_truth_combined.csv\n",
            "                            GDC_Aliquot Chromosome  Start        End  \\\n",
            "0  2eb1196d-5e6d-4223-953c-d558c75a8c2e       chr1  13116  248945703   \n",
            "1  2eb1196d-5e6d-4223-953c-d558c75a8c2e       chr2  10587  242183243   \n",
            "2  2eb1196d-5e6d-4223-953c-d558c75a8c2e       chr3  18519  198181744   \n",
            "3  2eb1196d-5e6d-4223-953c-d558c75a8c2e       chr4  11961  190122722   \n",
            "4  2eb1196d-5e6d-4223-953c-d558c75a8c2e       chr5  11882  181363900   \n",
            "\n",
            "   Copy_Number  Major_Copy_Number  Minor_Copy_Number    Case_ID  Sample  \n",
            "0            4                  2                  2  C3L-00359       1  \n",
            "1            4                  2                  2  C3L-00359       1  \n",
            "2            4                  2                  2  C3L-00359       1  \n",
            "3            4                  2                  2  C3L-00359       1  \n",
            "4            4                  2                  2  C3L-00359       1  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CLkhwI3O4nTj"
      },
      "source": [
        "Or retrieve combined CNV data locally"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cvQc8hEuP8Zf"
      },
      "outputs": [],
      "source": [
        "file_path = \"D:/GDC-data/ground_truth_combined.csv\"\n",
        "df = pd.read_csv(file_path, delimiter=',')\n",
        "print(df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3KBWhXgOPg58"
      },
      "source": [
        "Gene data retrieval function using PyEnsembl\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "DOMZE0QolbdN"
      },
      "outputs": [],
      "source": [
        "# use latest Ensembl release for human genome\n",
        "data = EnsemblRelease(84)\n",
        "data.download()\n",
        "data.index()\n",
        "\n",
        "# function to get gene data for the locus\n",
        "def get_gene_data(chromosome, start, end):\n",
        "    genes = data.genes_at_locus(contig=chromosome, position=start, end=end)\n",
        "    if not genes:\n",
        "        return None, None, None, None\n",
        "\n",
        "    # Choose the longest gene at the locus\n",
        "    prominent_gene = max(genes, key=lambda gene: gene.end - gene.start)\n",
        "\n",
        "    gene_name = prominent_gene.name\n",
        "    gene_biotype = prominent_gene.biotype\n",
        "    gene_length = prominent_gene.end - prominent_gene.start\n",
        "    exon_count = len(prominent_gene.exons)\n",
        "\n",
        "    return gene_name, gene_biotype, gene_length, exon_count"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XlqwaKhCP8Zi"
      },
      "source": [
        "Append gene data columns to existing dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": true
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1dPu0RgMP8Zj",
        "outputId": "630ce1e8-3f0a-435e-e73b-94ee92b8fc99"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                            GDC_Aliquot Chromosome     Start        End  \\\n",
            "0  2eb1196d-5e6d-4223-953c-d558c75a8c2e       chr1     13116  248945703   \n",
            "1  2eb1196d-5e6d-4223-953c-d558c75a8c2e       chr2     10587  242183243   \n",
            "2  2eb1196d-5e6d-4223-953c-d558c75a8c2e       chr3     18519  198181744   \n",
            "3  2eb1196d-5e6d-4223-953c-d558c75a8c2e       chr4     11961  190122722   \n",
            "4  2eb1196d-5e6d-4223-953c-d558c75a8c2e       chr5     11882  181363900   \n",
            "5  2eb1196d-5e6d-4223-953c-d558c75a8c2e       chr6    100116   32468178   \n",
            "6  2eb1196d-5e6d-4223-953c-d558c75a8c2e       chr6  32469273   32536402   \n",
            "7  2eb1196d-5e6d-4223-953c-d558c75a8c2e       chr6  32537412  170740469   \n",
            "8  2eb1196d-5e6d-4223-953c-d558c75a8c2e       chr7     20608  159334386   \n",
            "9  2eb1196d-5e6d-4223-953c-d558c75a8c2e       chr8     61774  125589296   \n",
            "\n",
            "   Copy_Number  Major_Copy_Number  Minor_Copy_Number    Case_ID  Sample  \\\n",
            "0            4                  2                  2  C3L-00359       1   \n",
            "1            4                  2                  2  C3L-00359       1   \n",
            "2            4                  2                  2  C3L-00359       1   \n",
            "3            4                  2                  2  C3L-00359       1   \n",
            "4            4                  2                  2  C3L-00359       1   \n",
            "5            4                  2                  2  C3L-00359       1   \n",
            "6            5                  4                  1  C3L-00359       1   \n",
            "7            4                  2                  2  C3L-00359       1   \n",
            "8            4                  2                  2  C3L-00359       1   \n",
            "9            4                  2                  2  C3L-00359       1   \n",
            "\n",
            "  gene_name gene_biotype gene_length exon_count  \n",
            "0      None         None        None       None  \n",
            "1      None         None        None       None  \n",
            "2      None         None        None       None  \n",
            "3      None         None        None       None  \n",
            "4      None         None        None       None  \n",
            "5      None         None        None       None  \n",
            "6      None         None        None       None  \n",
            "7      None         None        None       None  \n",
            "8      None         None        None       None  \n",
            "9      None         None        None       None  \n"
          ]
        }
      ],
      "source": [
        "# apply function to each row in dataframe\n",
        "df[['gene_name', 'gene_biotype', 'gene_length', 'exon_count']] = df.apply(\n",
        "    lambda row: pd.Series(get_gene_data(row['Chromosome'], row['Start'], row['End'])),\n",
        "    axis=1\n",
        ")\n",
        "\n",
        "print(df.head(10)) # updated dataframe"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "czrRXKFBztzu"
      },
      "source": [
        "### Gene Data Selection\n",
        "\n",
        "Specific gene data was chosen to add relevant features to the current ground truth table\n",
        "\n",
        "- Gene name: for information, common name for interpreting data\n",
        "- Gene biotype: type of gene, can indicate if the CNV affects protein-coding genes\n",
        "- Gene length: measurement to check for patterns among CNV segments\n",
        "- Exon count: checking if the CNV affects specific regions of a gene"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jluKOm95P8Zk"
      },
      "source": [
        "### Additional Notes\n",
        "- PyEnsembl's `gene_at_locus()` returns all genes overlapping at that locus.\n",
        "- We will map the most \"prominent\" gene to the corresponding row of the ground truth dataframe.\n",
        "- Currently the criteria for the prominent gene is the longest gene at the locus (see the lambda function for returning the max gene length)."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}